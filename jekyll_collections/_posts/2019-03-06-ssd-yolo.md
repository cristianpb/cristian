---
layout: post
title: "Object detection using a Raspberry Pi with Yolo and SSD Mobilenet"
date: 2019-03-06
description: "This post how how to implement a light object detection algorithm"
categories:
  - data science
  - programming
tags: opencv raspberrypi python
image:
  path: /assets/img/ssd-yolo/main-crop.jpg
  height: 200
  width: 300
ligthbox: true
video: true

---

Deep learning algorithms are very useful for computer vision in applications
such as image classification, object detection, or instance segmentation.
The main drawback is that these algorithms need in most cases graphical
processing units to be trained and sometimes making predictions can require to
load a heavy model. 
This is one of the main constrains when putting a deep learning model in production.

In this post, I will explain how to use state of the art deep learning
algorithms for object detection that can run in light environments such as a Raspberrypi.

<amp-image-lightbox id="lightbox1"
  layout="nodisplay"></amp-image-lightbox>
<amp-img on="tap:lightbox1"
  role="button"
  tabindex="0"
  aria-describedby="imageDescription2"
  alt="Picture of a dog"
  title="Picture of a dog, view in lightbox"
  src="/assets/img/ssd-yolo/yolo-big.jpg"
  layout="responsive"
  width="2500"
  height="1600"></amp-img>
<div id="imageDescription2">
  Object detection using Yolo V3
</div>

## OpenCV DNN module

Most recent deep learning models are trained either in Tensorflow or Pytorch.
Training a model requires to determine a high number of parameters, but not of
them are used when doing inference (predictions). 
Tensorflow's way to export a model is basically to identify the parameters that
are needed for inference (graph, weights, etc) and export them in a Google
optimized format called [Protobuf](https://github.com/protocolbuffers/protobuf)

This *protobuf* model is very light compared to the original Tensorflow model
and can be used for simple inference tasks. Google team released a 
[model zoo repository](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)
with trained and optimized models that can be use for object detection applications. 

Additionally, computer vision libraries like OpenCV can handle protobuf files
to make predictions and remove tensorflow dependency when deploying a model.
The OpenCV module that make this possible is called [DNN module](https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API),
which implements forward pass for deep networks.

## Mobilenet SSD

One of the more used models for computer vision in light environments is
[Mobilenet](https://arxiv.org/abs/1704.04861). This convolutional model has a
trade-off between latency and accuracy.
It can be found in the Tensorflow object detection zoo, where you can 
[download the model and the configuration files](https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API#use-existing-config-file-for-your-model).
Now I will describe the main functions used for making predictions.

### Load tensorflow model

First we have to load the model into memory.
The function `readNetFromTensorflow` from the DNN OpenCV module loads the Tensorflow model and 
a frozen protobuf file to be used for out of the box inference.

```python
model = cv2.dnn.readNetFromTensorflow(
        'models/ssd_mobilenet/frozen_inference_graph.pb',
        'models/ssd_mobilenet/ssd_mobilenet_v2_coco_2018_03_29.pbtxt')
```

### Blob image

Then to obtain (correct) predictions from the model you need to pre-process
your data.  OpenCV DNN modules includes the function `blobFromImage` which
creates a 4-dimensional  blob from the image.  It can also resize, crop an
image, subtract mean values, scale values by a given factor, swap blue and red
channels and many mode. To know more about blobs there is [this good
reference](https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/).

```python
model.setInput(cv2.dnn.blobFromImage(image, size=(300, 300), swapRB=True))
output = model.forward()
```

### Filter detections

The output of the models corresponds to an array of size (1, 1, 100, 7).
We are interested in the results of the layer [0,0], where the dimension with 100 values corresponds
to the number of detected bounding boxes and 7 corresponds to the class id, the
confidence score and the bounding box coordinates.
We can then filter the bounding box by the confidence score.

```python
final_detection = list()
for detection in output[0, 0, :, :]:
    confidence = detection[2]
    if confidence > THRESHOLD:
        final_detection.append(detection)
```

## Real time detection on Raspberry pi

Loading Mobilenet in a modern laptop takes about 0.5 seconds and inference
takes 0.19 seconds. While loading Mobilenet in Raspberry takes 2.97 seconds in
average and inference time is about 2.31 seconds. Which in real-time gives the
following output.

<amp-video width="720"
  height="405"
  src="/assets/img/ssd-yolo/video.webm"
  poster="/assets/img/ssd-yolo/yolo-big.jpg"
  layout="responsive"
  controls
  loop
  autoplay>
  <div fallback>
    <p>Your browser doesn't support HTML5 video.</p>
  </div>
</amp-video>

## Yolo V3

There are other light deep learning networks that performs well in object
detection like YOLO detection system, which model can be found on the [official
page](https://pjreddie.com/darknet/yolo/).
YOLOv3 is described as "extremely fast and accurate". 
Which is true, because loading a model the tiny version takes 0.091 seconds and inference takes 0.2 seconds. 
However, from my test, Mobilenet performs a little bit better, like you can see in the following pictures.

<div class="columns">
<div class="column">
<amp-image-lightbox id="lightbox2"
  layout="nodisplay"></amp-image-lightbox>
<amp-img on="tap:lightbox2"
  role="button"
  tabindex="0"
  aria-describedby="imageDescription"
  alt="yolo tiny predictions in a sample image"
  title="yolo tiny predictions in a sample image"
  src="/assets/img/ssd-yolo/yolo-tiny.jpg"
  layout="responsive"
  width="2500"
  height="1600"></amp-img>
<div id="imageDescription">
  Object detection using Tiny YoloV3
</div>
</div>
<div class="column">
<amp-img on="tap:lightbox2"
  role="button"
  tabindex="0"
  aria-describedby="imageDescription3"
  alt="ssd Mobilenet predictions in a sample image"
  title="ssd Mobilenet predictions in a sample image"
  src="/assets/img/ssd-yolo/ssd.jpg"
  layout="responsive"
  width="2500"
  height="1600"></amp-img>
<div id="imageDescription3">
  Object detection using SSD
</div>
</div>
</div>

The results of my tests can be found in the following table:

Device | Mobilenet  | | Yolo |
---|---|---
 | Model loading | Inference | Model loading | Inference
PC | 0.5 | 0.19 | 0.091 | 0.2
Raspberry | **2.97** | **2.31** | 0.6 | 3.0


## Conclusion

Using deep learning models in small environments like a Raspberrypi is possible and getting close to *real time* measurements.
The complete code can be found [here for the Mobilenet model](https://github.com/cristianpb/object-detection/blob/master/backend/ssd_detection.py)
and [here for yolo](https://github.com/cristianpb/object-detection/blob/master/backend/yolo_detection.py)
